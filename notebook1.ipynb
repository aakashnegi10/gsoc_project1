{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91019a15-64a1-402b-944e-a76083360616",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Working on GSOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1219d12-810a-44cc-a66b-2b2db94c6063",
   "metadata": {},
   "source": [
    "## Task 1: Social Media Data Extraction & Preprocessing (API Handling & Text Cleaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f611e-e866-462c-8777-e3583ba7da40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4584f-f3ff-449e-8ceb-47b789914099",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### My keys\n",
    "\n",
    "API_KEY = \"MQl6JqRK2YTb1AQg1leAiu6ET\"\n",
    "API_SECRET = \"GSq3rZN0d86zkBDx6s8JkI5ZkAIaHJ5897hlwluy4ibbLNLlcM\"\n",
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAGRZZgEAAAAASCZ1Mw9xjXI6cyc7RMI9Lvurv%2FY%3DOtmBNmZk8Qgw1NaQ2wXtGfkICUU1Ng7mhbPDscuiqGiqdPEU3z\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb45fc-1626-4f5a-9b47-f413e5ac06f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### making a test request to verify authentication\n",
    "\n",
    "import tweepy\n",
    "\n",
    "bearer_token = BEARER_TOKEN\n",
    "\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "try:\n",
    "    client.get_user(username=\"TwitterDev\")  # Test request\n",
    "    print(\"API Authentication Successful\")\n",
    "except Exception as e:\n",
    "    print(f\" Authentication Failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325f4ac-c7d4-4248-abdc-3d421a94915f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Authenticate with Twitter API\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3a1e5-ca4b-4862-9313-4a13767354bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### some widely recognized terms in mental health discussions\n",
    "\n",
    "keywords = [\n",
    "    \"depression\",\n",
    "    \"anxiety disorder\",\n",
    "    \"bipolar disorder\",\n",
    "    \"post-traumatic stress disorder\",\n",
    "    \"obsessive-compulsive disorder\",\n",
    "    \"schizophrenia\",\n",
    "    \"eating disorder\",\n",
    "    \"substance use disorder\",\n",
    "    \"self-harm\",\n",
    "    \"suicidal ideation\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38d320-0f20-47f4-a8c1-43d0f5721f74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Constructing the Search Query\n",
    "\n",
    "query = \" OR \".join(keywords) + \" -is:retweet lang:en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab048a-b58b-45a5-86de-6fac4c48fcd1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Retry logic for handling rate limits or temporary failures\n",
    "\n",
    "max_retries = 5  # Number of retry attempts\n",
    "retry_delay = 15  # Initial delay in seconds\n",
    "\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        # Fetch tweets (max 100 per request)\n",
    "        tweets = client.search_recent_tweets(\n",
    "            query=query,\n",
    "            tweet_fields=[\"id\", \"text\", \"created_at\", \"public_metrics\"],\n",
    "            max_results=100\n",
    "        )\n",
    "        break  # If successful, exit loop\n",
    "\n",
    "    except tweepy.TooManyRequests:  # Handle rate limits\n",
    "        print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds...\")\n",
    "        time.sleep(retry_delay)\n",
    "        retry_delay *= 2  # Exponential backoff\n",
    "\n",
    "    except tweepy.TweepyException as e:  # Catch general Tweepy errors\n",
    "        print(f\"Error: {e}\")\n",
    "        time.sleep(retry_delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab319d-9f1f-4a5d-9248-28d2a8a31bd1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Store extracted data\n",
    "data = []\n",
    "for tweet in tweets.data:\n",
    "    data.append({\n",
    "        \"Post ID\": tweet.id,\n",
    "        \"Timestamp\": tweet.created_at,\n",
    "        \"Content\": tweet.text,\n",
    "        \"Likes\": tweet.public_metrics[\"like_count\"],\n",
    "        \"Replies\": tweet.public_metrics[\"reply_count\"],\n",
    "        \"Retweets\": tweet.public_metrics[\"retweet_count\"]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810f71b-ed22-48bd-883d-6d30e5c07ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "610d3301-936d-47c0-a004-e1f5ec780a88",
   "metadata": {},
   "source": [
    "### was getting requests issues, hence shifted to reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482c7f5-3b30-446e-9bc1-f698ed74c356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb335efb-1960-4b02-878a-7b6cbe87c707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d217c9e1-00d5-4306-882c-e6368176118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aakash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Reddit Data Extraction\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81616de2-cdc4-4ba6-99c5-251ee48eb7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Reddit API connection\n",
    "\n",
    "user_agent= \"user_agent\"  #no space\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"-ggPSBpmj6tuwdfm-Av6vQ\",\n",
    "    client_secret=\"hP__xX396bg68yxyS9GzsCogUywMUQ\",\n",
    "    user_agent=user_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "185a8e2a-5e79-4702-9545-ef82887c3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "### some widely recognized terms in mental health discussions\n",
    "\n",
    "keywords = [\n",
    "    \"depression\",\n",
    "    \"anxiety disorder\",\n",
    "    \"bipolar disorder\",\n",
    "    \"post-traumatic stress disorder\",\n",
    "    \"obsessive-compulsive disorder\",\n",
    "    \"schizophrenia\",\n",
    "    \"eating disorder\",\n",
    "    \"substance use disorder\",\n",
    "    \"self-harm\",\n",
    "    \"suicidal ideation\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d35b47-8190-4ac3-b86a-558c425ab7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subreddits to extract from\n",
    "subreddit_name = [\n",
    "    \"mentalhealth\", \"depression\", \"addiction\", \"SuicideWatch\",\n",
    "    \"traumatoolbox\", \"socialanxiety\", \"Anger\", \"offmychest\", \n",
    "    \"bodyacceptance\", \"mentalhealthmemes\", \n",
    "    \"nosurf\", \"mentalhealth\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f47a3d7-4fad-44f8-bf4b-d682c372cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def is_subreddit_accessible(subreddit_name):\n",
    "    url = f\"https://www.reddit.com/r/{subreddit_name}/about.json\"\n",
    "    headers = {\"User-Agent\": \"your_user_agent\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return True  # ✅ Subreddit is accessible\n",
    "    else:\n",
    "        print(f\"❌ Subreddit r/{subreddit_name} is private, banned, or doesn't exist.\")\n",
    "        return False  # ❌ Remove this subreddit from the list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c601bad-3a54-4055-be42-8e1ac9bb5c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Subreddit r/mentalhealthmemes is private, banned, or doesn't exist.\n",
      "✅ Valid subreddits: ['mentalhealth', 'depression', 'addiction', 'SuicideWatch', 'traumatoolbox', 'socialanxiety', 'Anger', 'offmychest', 'bodyacceptance', 'nosurf', 'mentalhealth']\n"
     ]
    }
   ],
   "source": [
    "# ✅ Filter only accessible subreddits\n",
    "\n",
    "\n",
    "valid_subreddits = [sub for sub in subreddit_name if is_subreddit_accessible(sub)]\n",
    "print(f\"✅ Valid subreddits: {valid_subreddits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe4ee29a-8821-472b-a769-1166d619ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = emoji.replace_emoji(text, '')  # Correct way to remove emojis\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d150d02-c28f-40eb-a753-0d13e3423739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_posts(valid_subreddits, limit=100):\n",
    "    all_posts = []\n",
    "    \n",
    "    for subreddit_name in valid_subreddits:\n",
    "        print(f\"🔹 Fetching posts from r/{subreddit_name}...\")\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        for post in subreddit.hot(limit=limit):  # Fetch top 'hot' posts\n",
    "            if any(keyword in post.title.lower() or keyword in post.selftext.lower() for keyword in keywords):\n",
    "                all_posts.append({\n",
    "                    \"subreddit\": subreddit_name,  # ✅ Store the subreddit name\n",
    "                    \"post_id\": post.id,\n",
    "                    \"timestamp\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    \"content\": clean_text(post.title + ' ' + post.selftext),\n",
    "                    \"upvotes\": post.score,\n",
    "                    \"comments\": post.num_comments,\n",
    "                    \"shares\": post.num_crossposts\n",
    "                })\n",
    "    \n",
    "    return all_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083b2df-3273-4b73-9665-4a0a338a72cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "896056dd-8f99-4205-9e08-11f7cdb76827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Fetching posts from r/mentalhealth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22197/421323477.py:13: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  \"timestamp\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Fetching posts from r/depression...\n",
      "🔹 Fetching posts from r/addiction...\n",
      "🔹 Fetching posts from r/SuicideWatch...\n",
      "🔹 Fetching posts from r/traumatoolbox...\n",
      "🔹 Fetching posts from r/socialanxiety...\n",
      "🔹 Fetching posts from r/Anger...\n",
      "🔹 Fetching posts from r/offmychest...\n",
      "🔹 Fetching posts from r/bodyacceptance...\n",
      "🔹 Fetching posts from r/nosurf...\n",
      "🔹 Fetching posts from r/mentalhealth...\n",
      "✅ Collected 108 posts.\n"
     ]
    }
   ],
   "source": [
    "# Fetch posts\n",
    "posts_data = get_reddit_posts(valid_subreddits, limit=100)\n",
    "print(f\"✅ Collected {len(posts_data)} posts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c297e2-d097-48c7-84c2-f3cca4300491",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8110a71-ddb4-475f-9f3b-2bce4bd40fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data saved as reddit_posts.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert the collected data to a DataFrame\n",
    "df = pd.DataFrame(posts_data)\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "filename = \"reddit_posts.csv\"\n",
    "df.to_csv(filename, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Data saved as {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d27acc-05b0-4ad6-8521-76519cad163f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdfcbccf-9afd-4a07-a8b7-baa5468e338f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>content</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmfpse</td>\n",
       "      <td>2025-03-29 05:50:57</td>\n",
       "      <td>girlfriend undergoing ect im 23m ive known gir...</td>\n",
       "      <td>41</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmheax</td>\n",
       "      <td>2025-03-29 07:55:47</td>\n",
       "      <td>think everyday tw read selfharm suicidal thoug...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmku7i</td>\n",
       "      <td>2025-03-29 12:07:07</td>\n",
       "      <td>stop selfharm stopped cutting years ago cant s...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmglfq</td>\n",
       "      <td>2025-03-29 06:54:51</td>\n",
       "      <td>see psych hey yall ive currently got issue par...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmmpb3</td>\n",
       "      <td>2025-03-29 13:50:21</td>\n",
       "      <td>feel void emptiness inside seem go away 21f ep...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit  post_id            timestamp  \\\n",
       "0  mentalhealth  1jmfpse  2025-03-29 05:50:57   \n",
       "1  mentalhealth  1jmheax  2025-03-29 07:55:47   \n",
       "2  mentalhealth  1jmku7i  2025-03-29 12:07:07   \n",
       "3  mentalhealth  1jmglfq  2025-03-29 06:54:51   \n",
       "4  mentalhealth  1jmmpb3  2025-03-29 13:50:21   \n",
       "\n",
       "                                             content  upvotes  comments  \\\n",
       "0  girlfriend undergoing ect im 23m ive known gir...       41        47   \n",
       "1  think everyday tw read selfharm suicidal thoug...        5         1   \n",
       "2  stop selfharm stopped cutting years ago cant s...        2         5   \n",
       "3  see psych hey yall ive currently got issue par...        3         3   \n",
       "4  feel void emptiness inside seem go away 21f ep...        1         1   \n",
       "\n",
       "   shares  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b562ff14-26d0-4160-8039-7474e93881eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5b63d14-ad15-4940-ac25-b7fdabf5815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108 entries, 0 to 107\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  108 non-null    object\n",
      " 1   post_id    108 non-null    object\n",
      " 2   timestamp  108 non-null    object\n",
      " 3   content    108 non-null    object\n",
      " 4   upvotes    108 non-null    int64 \n",
      " 5   comments   108 non-null    int64 \n",
      " 6   shares     108 non-null    int64 \n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 6.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c909472-81a0-4244-9966-bfd4d4f6dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit    0\n",
      "post_id      0\n",
      "timestamp    0\n",
      "content      0\n",
      "upvotes      0\n",
      "comments     0\n",
      "shares       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())  # Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66580265-30ef-4323-af1a-43bc743840eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      108.000000\n",
      "mean      1573.055556\n",
      "std       2214.702933\n",
      "min         49.000000\n",
      "25%        553.000000\n",
      "50%        859.000000\n",
      "75%       1635.000000\n",
      "max      14410.000000\n",
      "Name: content_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['content_length'] = df['content'].apply(len)\n",
    "print(df['content_length'].describe())  # Get min, max, mean length of posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3b6db-a4e0-4750-9f16-7aa1e316add1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e6b5d3a-4bb1-4277-aa40-e9b2ae7bd0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     subreddit  post_id            timestamp  \\\n",
      "15  depression   doqwow  2019-10-29 14:52:02   \n",
      "85  offmychest  1jlreto  2025-03-28 09:50:15   \n",
      "38  depression  1jlk9ub  2025-03-28 02:02:08   \n",
      "17  depression  1jm9yrj  2025-03-29 00:24:32   \n",
      "20  depression  1jm35st  2025-03-28 19:19:08   \n",
      "\n",
      "                                              content  upvotes  comments  \\\n",
      "15  mostbroken leastunderstood rules helpers may i...     2364       177   \n",
      "85  spent 1600 savings porn 2 hours regret throwaw...     1814       299   \n",
      "38  tonight im going kill name gian im peru unfort...      451       129   \n",
      "17  dad said depression choice choosing depression...      132        89   \n",
      "20  possible overcome depression im tired depresse...       97        51   \n",
      "\n",
      "    shares  content_length  \n",
      "15       2            3202  \n",
      "85       0            1195  \n",
      "38       0             939  \n",
      "17       0              97  \n",
      "20       0             142  \n",
      "     subreddit  post_id            timestamp  \\\n",
      "85  offmychest  1jlreto  2025-03-28 09:50:15   \n",
      "16  depression  1frqlk0  2024-09-28 23:05:43   \n",
      "15  depression   doqwow  2019-10-29 14:52:02   \n",
      "38  depression  1jlk9ub  2025-03-28 02:02:08   \n",
      "17  depression  1jm9yrj  2025-03-29 00:24:32   \n",
      "\n",
      "                                              content  upvotes  comments  \\\n",
      "85  spent 1600 savings porn 2 hours regret throwaw...     1814       299   \n",
      "16  regular checkin post information rules wikis w...       45       268   \n",
      "15  mostbroken leastunderstood rules helpers may i...     2364       177   \n",
      "38  tonight im going kill name gian im peru unfort...      451       129   \n",
      "17  dad said depression choice choosing depression...      132        89   \n",
      "\n",
      "    shares  content_length  \n",
      "85       0            1195  \n",
      "16       0            1585  \n",
      "15       2            3202  \n",
      "38       0             939  \n",
      "17       0              97  \n"
     ]
    }
   ],
   "source": [
    "print(df.sort_values(by='upvotes', ascending=False).head(5))  # Top 5 upvoted\n",
    "print(df.sort_values(by='comments', ascending=False).head(5))  # Top 5 commented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5bbe59-36b7-4aa7-a04f-dc1ac4e73b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21055f-8cb9-46d0-98bb-281967f93540",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Cleaning the Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82a582aa-5fcc-4548-843c-638e7758fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aakash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aakash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aakash/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.data.path.append('/home/aakash/nltk_data')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68834e34-4495-4b56-8a9a-6d18aed1fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 🔹 Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 🔹 Function to clean text\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Apply lemmatization\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31c83069-617d-499d-9168-406ea46bede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aakash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('/home/aakash/nltk_data')\n",
    "nltk.download('punkt', download_dir='/home/aakash/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e7ec590-93ab-427c-aad3-b82ab2580459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      girlfriend undergoing ect im 23m ive known gir...\n",
       "1      think everyday tw read selfharm suicidal thoug...\n",
       "2      stop selfharm stopped cutting years ago cant s...\n",
       "3      see psych hey yall ive currently got issue par...\n",
       "4      feel void emptiness inside seem go away 21f ep...\n",
       "                             ...                        \n",
       "103    seeing hate affecting mental health noticed be...\n",
       "104    clozapine escitalopram anxiety anyone share ex...\n",
       "105    depression starting times really feel chance p...\n",
       "106    mental health issues caused single lonely mala...\n",
       "107    im point food thing makes happy life ive strug...\n",
       "Name: content, Length: 108, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2899ea33-98fd-46f2-9445-0b9633aa9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset saved as cleaned_reddit_posts.csv\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Apply cleaning to dataset\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# 🔹 Save cleaned dataset\n",
    "df.to_csv(\"cleaned_reddit_posts.csv\", index=False)\n",
    "print(\"✅ Cleaned dataset saved as cleaned_reddit_posts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "227cdf03-418f-4063-a3aa-1cff2ccc56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset saved with only cleaned content.\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Drop the old content column\n",
    "df = df.drop(columns=['content'])\n",
    "\n",
    "# 🔹 Save the cleaned dataset\n",
    "df.to_csv(\"cleaned_reddit_posts.csv\", index=False)\n",
    "print(\"✅ Cleaned dataset saved with only cleaned content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26db02a0-0c5e-44dd-a12a-2597891e2096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "      <th>content_length</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmfpse</td>\n",
       "      <td>2025-03-29 05:50:57</td>\n",
       "      <td>41</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>747</td>\n",
       "      <td>girlfriend undergoing ect im ive known girlfri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmheax</td>\n",
       "      <td>2025-03-29 07:55:47</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>978</td>\n",
       "      <td>think everyday tw read selfharm suicidal thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmku7i</td>\n",
       "      <td>2025-03-29 12:07:07</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>stop selfharm stopped cutting year ago cant st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmglfq</td>\n",
       "      <td>2025-03-29 06:54:51</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>550</td>\n",
       "      <td>see psych hey yall ive currently got issue par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>1jmmpb3</td>\n",
       "      <td>2025-03-29 13:50:21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>864</td>\n",
       "      <td>feel void emptiness inside seem go away f epis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit  post_id            timestamp  upvotes  comments  shares  \\\n",
       "0  mentalhealth  1jmfpse  2025-03-29 05:50:57       41        47       0   \n",
       "1  mentalhealth  1jmheax  2025-03-29 07:55:47        5         1       0   \n",
       "2  mentalhealth  1jmku7i  2025-03-29 12:07:07        2         5       0   \n",
       "3  mentalhealth  1jmglfq  2025-03-29 06:54:51        3         3       0   \n",
       "4  mentalhealth  1jmmpb3  2025-03-29 13:50:21        1         1       0   \n",
       "\n",
       "   content_length                                    cleaned_content  \n",
       "0             747  girlfriend undergoing ect im ive known girlfri...  \n",
       "1             978  think everyday tw read selfharm suicidal thoug...  \n",
       "2             121  stop selfharm stopped cutting year ago cant st...  \n",
       "3             550  see psych hey yall ive currently got issue par...  \n",
       "4             864  feel void emptiness inside seem go away f epis...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b956403-1dca-42ee-86dc-737885bade43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7037949f-8644-4f16-8c49-9f04da0057da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     108.000000\n",
      "mean      225.805556\n",
      "std       303.378837\n",
      "min         9.000000\n",
      "25%        88.000000\n",
      "50%       126.000000\n",
      "75%       241.000000\n",
      "max      1940.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['word_count'] = df['cleaned_content'].apply(lambda x: len(x.split()))\n",
    "print(df['word_count'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647112dd-4c21-4692-828e-9ebadfe01961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63dbd696-4a4c-4a46-8701-8d2001cd8019",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bade9-946e-4ef8-b6d8-19f5664a122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 🔹 Step 1: Sentiment Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb289a1-6a0c-4f3d-829f-7bc40a0e4106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315eadb-df1c-49cf-833b-dda317c5e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Load pre-trained BERT sentiment model (DistilBERT-SST-2)\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443b48b-e870-496e-88ad-5f079ae2ab63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55eaa9-19c7-42c9-912a-67bbe5d3a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 🔹 Function to classify sentiment\n",
    "def classify_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment = \"Positive\" if torch.argmax(probs) == 1 else \"Negative\"\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521e4d3-3486-4f22-ac83-5914871083b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec8f76-797c-43ba-b810-28cf69094018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 🔹 Load your cleaned Reddit dataset\n",
    "df = pd.read_csv(\"cleaned_reddit_posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91137f-fd79-4aa5-a4d1-9b491b741287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbadd5-12db-4e85-b6f4-0be8a0e79975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd160e23-f73a-4402-b499-53e27e3ac4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ba6db-a482-470b-8466-286f24c3020f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb014207-054b-44dc-aa65-2535ab4b71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Crisis Risk Classification (TF-IDF & Rule-Based Detection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17082f-cf05-4c1b-80d5-5d801f169ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c6403-2ba0-4715-8ff3-1c0a093c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk_terms = [\n",
    "    \"i don’t want to be here\", \"i want to end it\", \"life is meaningless\", \n",
    "    \"no reason to live\", \"i can’t go on\", \"goodbye everyone\", \"ending it all\"\n",
    "]\n",
    "\n",
    "moderate_risk_terms = [\n",
    "    \"i feel lost\", \"struggling\", \"overwhelmed\", \"i need help\", \"nobody understands\", \n",
    "    \"depressed\", \"can’t cope\", \"feeling empty\"\n",
    "]\n",
    "\n",
    "# Function to classify crisis risk level\n",
    "def classify_risk(text):\n",
    "    text_lower = text.lower()\n",
    "    if any(term in text_lower for term in high_risk_terms):\n",
    "        return \"High-Risk\"\n",
    "    elif any(term in text_lower for term in moderate_risk_terms):\n",
    "        return \"Moderate Concern\"\n",
    "    else:\n",
    "        return \"Low Concern\"\n",
    "\n",
    "# Apply risk classification\n",
    "df[\"Risk_Level\"] = df[\"Cleaned_Content\"].apply(classify_risk)\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv(\"classified_twitter_data.csv\", index=False)\n",
    "print(\"✅ Crisis risk classification done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b2681-7315-437a-b667-9a26a8df7169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2baa2d-10b6-44db-9451-666546bbf1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e951226-f066-4f47-8243-bd38f6d90c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a237e-3471-4fc0-a493-6e05f41e7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 🔹 Step 3: Visualizing Sentiment & Risk Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fadb98-d9f1-4bd7-94ae-f5045beaa909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9a738-ada9-4c8c-bc8d-2e44a42ba7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(x=df[\"Sentiment\"], palette=\"coolwarm\")\n",
    "plt.title(\"Sentiment Analysis Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Plot risk level distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(x=df[\"Risk_Level\"], palette=\"Reds\")\n",
    "plt.title(\"Crisis Risk Classification Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048bf0a-b540-4cec-b710-52fc15b2948c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b03a8-3587-4311-971f-37dd25811d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e1bb5-a94f-4cc0-b461-3e29cd412d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade2aa5-2080-4057-b700-91c7da13f9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6e0ee-fe2c-449b-8381-0b1feee225b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extracting Location Data\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12615738-44e9-41b3-b7c6-a6864df965e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check available location data\n",
    "print(df[\"Location\"].value_counts().head(10))  # View top locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64614a76-54c0-435d-a6eb-31a7aa9c7fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ccdbb-b4c4-460c-8e7f-711dd1442905",
   "metadata": {},
   "outputs": [],
   "source": [
    "###📌 Extract Locations from Post Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a573aa-65af-43d5-bde7-6bf9470bf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract locations\n",
    "def extract_location(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":  # GPE = Geo-Political Entity (cities, countries, etc.)\n",
    "            return ent.text\n",
    "    return None  # No location found\n",
    "\n",
    "# Apply location extraction\n",
    "df[\"Extracted_Location\"] = df[\"Cleaned_Content\"].apply(extract_location)\n",
    "\n",
    "# View extracted locations\n",
    "print(df[\"Extracted_Location\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d6be2b-24f5-4541-b189-a62ac443f995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd101f02-7a14-4ddf-8d1f-b22822e2a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Geocoding (Convert Locations to Lat/Lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309a823-2166-4b37-8b27-c8aecf326c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geo_locator\")\n",
    "\n",
    "# Function to get latitude & longitude\n",
    "def get_coordinates(location):\n",
    "    if pd.isna(location):\n",
    "        return None, None\n",
    "    try:\n",
    "        time.sleep(1)  # To avoid API rate limits\n",
    "        loc = geolocator.geocode(location)\n",
    "        if loc:\n",
    "            return loc.latitude, loc.longitude\n",
    "    except:\n",
    "        return None, None\n",
    "    return None, None\n",
    "\n",
    "# Apply geocoding\n",
    "df[[\"Latitude\", \"Longitude\"]] = df[\"Extracted_Location\"].apply(lambda x: pd.Series(get_coordinates(x)))\n",
    "\n",
    "# Remove rows where coordinates are missing\n",
    "df = df.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
    "\n",
    "# Save the geocoded dataset\n",
    "df.to_csv(\"geocoded_twitter_data.csv\", index=False)\n",
    "print(\"✅ Geolocation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577415d4-bade-427d-a99a-5276450944ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f080a-a2ac-4295-9088-b719535f6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 🔹 Step 3: Creating a Crisis Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc372a-4d93-4d8c-a915-35ca1b6169e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e528b1-9662-46af-b3a3-c4e612d4fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Initialize map centered at an approximate location\n",
    "m = folium.Map(location=[20, 0], zoom_start=2)  # World view\n",
    "\n",
    "# Add heatmap layer\n",
    "heat_data = df[[\"Latitude\", \"Longitude\"]].values.tolist()\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "# Save and display map\n",
    "m.save(\"crisis_heatmap.html\")\n",
    "print(\"✅ Heatmap generated! Open 'crisis_heatmap.html' to view.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0d2d9-cdd7-423d-8105-03dc327fe7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025668a-57b6-4d15-9210-d1613378af66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971b50d-198a-4682-a353-d12d9e34e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🔹 Step 4: Finding Top 5 Crisis Locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261e5d5-f16f-4dee-be24-3f0a3fc509a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count top locations\n",
    "top_locations = df[\"Extracted_Location\"].value_counts().head(5)\n",
    "print(top_locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a9c93-1286-46b1-9c0d-8468b8e8f382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171cf40-2bec-4acf-90f5-f238955d2e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e49723-ec47-4a86-ab76-54ac2f979e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6ecc0-9e89-4536-898c-26c4685c7ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f5f4a-88c1-4861-89b2-3b6b7914edb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247860f-f34f-4451-86aa-2d00a15aa821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a80e1-d724-413d-87ae-798309fa1727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de2cfc-b1b0-4214-af80-ecd87000acf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
